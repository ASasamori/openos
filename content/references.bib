---
---

@book{arpaci2018operating,
  title={Operating systems: Three easy pieces},
  author={Arpaci-Dusseau, Remzi H and Arpaci-Dusseau, Andrea C},
  url = {https://pages.cs.wisc.edu/~remzi/OSTEP/},
  year={2018},
  publisher={Arpaci-Dusseau Books LLC Boston}
}

@inproceedings{forkinroad,
author = {Baumann, Andrew and Appavoo, Jonathan and Krieger, Orran and Roscoe, Timothy},
title = {A fork() in the road},
year = {2019},
isbn = {9781450367271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3317550.3321435},
doi = {10.1145/3317550.3321435},
abstract = {The received wisdom suggests that Unix's unusual combination of fork() and exec() for process creation was an inspired design. In this paper, we argue that fork was a clever hack for machines and programs of the 1970s that has long outlived its usefulness and is now a liability. We catalog the ways in which fork is a terrible abstraction for the modern programmer to use, describe how it compromises OS implementations, and propose alternatives.As the designers and implementers of operating systems, we should acknowledge that fork's continued existence as a first-class OS primitive holds back systems research, and deprecate it. As educators, we should teach fork as a historical artifact, and not the first process creation mechanism students encounter.},
booktitle = {Proceedings of the Workshop on Hot Topics in Operating Systems},
pages = {14–22},
numpages = {9},
location = {Bertinoro, Italy},
series = {HotOS '19}
}

@article{10.1145/1288783.1288788,
author = {Agrawal, Nitin and Bolosky, William J. and Douceur, John R. and Lorch, Jacob R.},
title = {A Five-Year Study of File-System Metadata},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {1553-3077},
url = {https://doi.org/10.1145/1288783.1288788},
doi = {10.1145/1288783.1288788},
abstract = {For five years, we collected annual snapshots of file-system metadata from over 60,000 Windows PC file systems in a large corporation. In this article, we use these snapshots to study temporal changes in file size, file age, file-type frequency, directory size, namespace structure, file-system population, storage capacity and consumption, and degree of file modification. We present a generative model that explains the namespace structure and the distribution of directory sizes. We find significant temporal trends relating to the popularity of certain file types, the origin of file content, the way the namespace is used, and the degree of variation among file systems, as well as more pedestrian changes in size and capacities. We give examples of consequent lessons for designers of file systems and related software.},
journal = {ACM Trans. Storage},
month = {oct},
pages = {9–es},
numpages = {32},
keywords = {File systems, generative model, longitudinal study}
}

@techreport{10.5555/1102034,
author = {Dijkstra, Edsger Wybe},
title = {Cooperating Sequential Processes, Technical Report EWD-123},
year = {1965}
}

@article{Peterson1981,
  title={Myths About the Mutual Exclusion Problem},
  author={Gary L. Peterson},
  journal={Information Processing Letters},
  year={1981},
  volume={12},
  pages={115-116}
}

@Inbook{Schneider1997,
author="Schneider, Fred B.",
title={On Concurrent Programming},
year="1997",
publisher="Springer New York NY",
isbn="978-0-387-94942-0",
doi="10.1007/978-1-4612-1830-2",
}

@article{10.1145/103727.103729,
author = {Mellor-Crummey, John M. and Scott, Michael L.},
title = {Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors},
year = {1991},
issue_date = {Feb. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/103727.103729},
doi = {10.1145/103727.103729},
abstract = {Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible flag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory.We present a new scalable algorithm for spin locks that generates 0(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than a swap-with-memory instruction. We also present a new scalable barrier algorithm that generates 0(1) remote references per processor reaching the barrier, and observe that two previously-known barriers can likewise be cast in a form that spins only on locally-accessible flag variables. None of these barrier algorithms requires hardware support beyond the usual atomicity of memory reads and writes.We compare the performance of our scalable algorithms with other software approaches to busy-wait synchronization on both a Sequent Symmetry and a BBN Butterfly. Our principal conclusion is that contention due to synchronization need not be a problem in large-scale shared-memory multiprocessors. The existence of scalable algorithms greatly weakens the case for costly special-purpose hardware support for synchronization, and provides a case against so-called “dance hall” architectures, in which shared memory locations are equally far from all processors. —From the Authors' Abstract},
journal = {ACM Trans. Comput. Syst.},
month = {feb},
pages = {21–65},
numpages = {45}
}

@article{10.1145/361082.361093,
author = {Lamport, Leslie},
title = {A New Solution of Dijkstra's Concurrent Programming Problem},
year = {1974},
issue_date = {Aug. 1974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/361082.361093},
doi = {10.1145/361082.361093},
abstract = {A simple solution to the mutual exclusion problem is presented which allows the system to continue to operate despite the failure of any individual component.},
journal = {Commun. ACM},
month = {aug},
pages = {453–455},
numpages = {3},
keywords = {concurrent programming, semaphores, multiprocessing, critical section}
}

@inproceedings{10.1145/1346281.1346323,
author = {Lu, Shan and Park, Soyeon and Seo, Eunsoo and Zhou, Yuanyuan},
title = {Learning from Mistakes: A Comprehensive Study on Real World Concurrency Bug Characteristics},
year = {2008},
isbn = {9781595939586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1346281.1346323},
doi = {10.1145/1346281.1346323},
abstract = {The reality of multi-core hardware has made concurrent programs pervasive. Unfortunately, writing correct concurrent programs is difficult. Addressing this challenge requires advances in multiple directions, including concurrency bug detection, concurrent program testing, concurrent programming model design, etc. Designing effective techniques in all these directions will significantly benefit from a deep understanding of real world concurrency bug characteristics.This paper provides the first (to the best of our knowledge) comprehensive real world concurrency bug characteristic study. Specifically, we have carefully examined concurrency bug patterns, manifestation, and fix strategies of 105 randomly selected real world concurrency bugs from 4 representative server and client open-source applications (MySQL, Apache, Mozilla and OpenOffice). Our study reveals several interesting findings and provides useful guidance for concurrency bug detection, testing, and concurrent programming language design.Some of our findings are as follows: (1) Around one third of the examined non-deadlock concurrency bugs are caused by violation to programmers' order intentions, which may not be easily expressed via synchronization primitives like locks and transactional memories; (2) Around 34% of the examined non-deadlock concurrency bugs involve multiple variables, which are not well addressed by existing bug detection tools; (3) About 92% of the examined concurrency bugs canbe reliably triggered by enforcing certain orders among no more than 4 memory accesses. This indicates that testing concurrent programs can target at exploring possible orders among every small groups of memory accesses, instead of among all memory accesses; (4) About 73% of the examinednon-deadlock concurrency bugs were not fixed by simply adding or changing locks, and many of the fixes were not correct at the first try, indicating the difficulty of reasoning concurrent execution by programmers.},
booktitle = {Proceedings of the 13th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {329–339},
numpages = {11},
keywords = {concurrency bug, bug characteristics, concurrent program},
location = {Seattle, WA, USA},
series = {ASPLOS XIII}
}

@article{10.1145/356586.356588,
author = {Coffman, E. G. and Elphick, M. and Shoshani, A.},
title = {System Deadlocks},
year = {1971},
issue_date = {June 1971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/356586.356588},
doi = {10.1145/356586.356588},
abstract = {A problem of increasing importance in the design of large multiprogramming systems is the, so-called, deadlock or deadly-embrace problem. In this article we survey the work that has been done on the treatment of deadlocks from both the theoretical and practical points of view.},
journal = {ACM Comput. Surv.},
month = {jun},
pages = {67–78},
numpages = {12}
}

@article{10.1145/356603.356607,
author = {Holt, Richard C.},
title = {Some Deadlock Properties of Computer Systems},
year = {1972},
issue_date = {Sept. 1972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/356603.356607},
doi = {10.1145/356603.356607},
journal = {ACM Comput. Surv.},
month = {sep},
pages = {179–196},
numpages = {18}
}



@inproceedings{10.1145/99163.99185,
author = {Herlihy, M.},
title = {A Methodology for Implementing Highly Concurrent Data Structures},
year = {1990},
isbn = {0897913507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/99163.99185},
doi = {10.1145/99163.99185},
abstract = {A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections: ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, non-faulty processes will be unable to progress. By contrast, a concurrent object implementation is non-blocking if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait-free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing non-blocking and wait-free implementations of concurrent objects. The object's representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is automatically transformed into a non-blocking or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying read, write, and compare&swap operations to a shared memory.},
booktitle = {Proceedings of the Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming},
pages = {197–206},
numpages = {10},
location = {Seattle, Washington, USA},
series = {PPOPP '90}
}

@article{10.1145/161468.161469,
author = {Herlihy, Maurice},
title = {A Methodology for Implementing Highly Concurrent Data Objects},
year = {1993},
issue_date = {Nov. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {0164-0925},
url = {https://doi.org/10.1145/161468.161469},
doi = {10.1145/161468.161469},
abstract = {A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections; ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, nonfaulty processes will be unable to progress. By contrast, a concurrent object implementation is lock free if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing lock-free and wait-free implementations of concurrent objects. The object's representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is atutomatically transformed into a lock-free or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying atomic read, write, load_linked, and store_conditional operations to a shared memory.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {nov},
pages = {745–770},
numpages = {26}
}

@book{10.5555/2655363,
author = {Tanenbaum, Andrew S. and Bos, Herbert},
title = {Modern Operating Systems},
year = {2014},
isbn = {013359162X},
publisher = {Prentice Hall Press},
address = {USA},
edition = {4th},
abstract = {Modern Operating Systems, Fourth Edition, is intended for introductory courses in Operating Systems in Computer Science, Computer Engineering, and Electrical Engineering programs. It also serves as a useful reference for OS professionals The widely anticipated revision of this worldwide best-seller incorporates the latest developments in operating systems (OS) technologies. The Fourth Edition includes up-to-date materials on relevantOS. Tanenbaum also provides information on current research based on his experience as an operating systems researcher. Modern Operating Systems, Third Editionwas the recipient of the 2010 McGuffey Longevity Award. The McGuffey Longevity Award recognizes textbooks whose excellence has been demonstrated over time.http://taaonline.net/index.html Teaching and Learning Experience This program will provide a better teaching and learning experiencefor you and your students. It will help: Provide Practical Detail on the Big Picture Concepts: A clear and entertaining writing style outlines the concepts every OS designer needs to master. Keep Your Course Current: This edition includes information on the latest OS technologies and developments Enhance Learning with Student and Instructor Resources: Students will gain hands-on experience using the simulation exercises and lab experiments.}
}

@article{10.1186/s13174-017-0055-2,
author = {Abbaspour Asadollah, S., and Sundmark, D., and Eldh, S., and Hansson, Hans},
title = {Concurrency bugs in open source software: a case study},
journal = {Journal of Internet Services and Applications},
number = {4},
volume = {8},
year = {2017},
month = {apr},
pages = {1-15},
issn = {1869-0238},
url = {https://doi.org/10.1186/s13174-017-0055-2},
doi = {10.1186/s13174-017-0055-2}
}

@article{10.1145/362759.362813,
author = {Courtois, P. J. and Heymans, F. and Parnas, D. L.},
title = {Concurrent Control with “Readers” and “Writers”},
year = {1971},
issue_date = {Oct. 1971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/362759.362813},
doi = {10.1145/362759.362813},
abstract = {The problem of the mutual exclusion of several independent processes from simultaneous access to a “critical section” is discussed for the case where there are two distinct classes of processes known as “readers” and “writers.” The “readers” may share the section with each other, but the “writers” must have exclusive access. Two solutions are presented: one for the case where we wish minimum delay for the readers; the other for the case where we wish writing to take place as early as possible.},
journal = {Commun. ACM},
month = {oct},
pages = {667–668},
numpages = {2},
keywords = {mutual exclusion, shared access to resources, critical section}
}

@article{10.1016/j.jpdc.2007.04.010,
author = {Hart, Thomas E. and McKenney, Paul E. and Brown, Angela Demke and Walpole, Jonathan},
title = {Performance of Memory Reclamation for Lockless Synchronization},
year = {2007},
issue_date = {December, 2007},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {67},
number = {12},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2007.04.010},
doi = {10.1016/j.jpdc.2007.04.010},
abstract = {Achieving high performance for concurrent applications on modern multiprocessors remains challenging. Many programmers avoid locking to improve performance, while others replace locks with non-blocking synchronization to protect against deadlock, priority inversion, and convoying. In both cases, dynamic data structures that avoid locking require a memory reclamation scheme that reclaims elements once they are no longer in use. The performance of existing memory reclamation schemes has not been thoroughly evaluated. We conduct the first fair and comprehensive comparison of three recent schemes-quiescent-state-based reclamation, epoch-based reclamation, and hazard-pointer-based reclamation-using a flexible microbenchmark. Our results show that there is no globally optimal scheme. When evaluating lockless synchronization, programmers and algorithm designers should thus carefully consider the data structure, the workload, and the execution environment, each of which can dramatically affect the memory reclamation performance. We discuss the consequences of our results for programmers and algorithm designers. Finally, we describe the use of one scheme, quiescent-state-based reclamation, in the context of an OS kernel-an execution environment which is well suited to this scheme.},
journal = {J. Parallel Distrib. Comput.},
month = {dec},
pages = {1270–1285},
numpages = {16},
keywords = {Performance, Non-blocking, Concurrency, Synchronization, Hazard pointers, Read-copy update, Memory reclamation, Lockless}
}
