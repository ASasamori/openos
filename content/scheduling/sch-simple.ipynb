{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0c1cd7-d2e7-44a1-a6b8-1f11ebe51a34",
   "metadata": {},
   "source": [
    "(cont:scheduling:scheduling:policies)=\n",
    "# Simple Examples of Scheduling Policies\n",
    "\n",
    "Now that we have a collection of requirements, let's look at a few simple possibilities and see which of our requirements they meet well and which ones they fail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae63f4-a6a6-4d1f-9727-eadc827cba1c",
   "metadata": {},
   "source": [
    "## First Come, First Served (FCFS)\n",
    "\n",
    "The simplest policy just processes each task to completion when they arrive. Just like waiting in line at the local government office, each process gets into a queue and the processor executes the first process in that queue until it completes. Then we repeat the same. \n",
    "\n",
    "One problem with this policy is that it can result in poor average turnaround time.  Imagine we have three tasks that arrive at around the same start time with the run time shown in the table below:\n",
    "\n",
    "| Task | Start | Runtime (min)     |\n",
    "| :--: | :---: | :----------: |\n",
    "| A    | 0     | 6            |\n",
    "| B    | 0     | 2            |\n",
    "| C    | 0     | 1            |\n",
    "\n",
    "If they are run in the order A, B, C, they will execute on the processor as shown below.  \n",
    "\n",
    "```{figure} ../images/scheduling/FIFO-1.png\n",
    "---\n",
    "name: VP:sched:FIFO\n",
    "---\n",
    "FIFO with tasks run in order A, then B, then C\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "So the average turnaround time is: $(6 + 8 + 9)/3 = 7.7 min$\n",
    "\n",
    "On the other hand, if the tasks are run in the order C, B, A, they will execute on the processor as shown below.  \n",
    "\n",
    "```{figure} ../images/scheduling/FIFO-2.png\n",
    "---\n",
    "name: VP:sched:FIFO2\n",
    "---\n",
    "FIFO with tasks run in order C, then B, then A\n",
    "```\n",
    "\n",
    "So the average turnaround time is: $(1 + 3 + 9)/3 = 4.7 min$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064f227-cda0-40a4-9870-4dc3d2e8ce22",
   "metadata": {},
   "source": [
    "## Shortest Job First (SJF)\n",
    "\n",
    "For systems like Batch, where we know a-priori how long a task will take, rather than running them in the order they arrived, we can sort them based on how long the tasks will take and always run the shortest tasks first to get the better turnaround time shown in  {numref}`VP:sched:FIFO2`.   This policy is called shortest job first, and will always yield the optimal turnaround time. \n",
    "\n",
    "Let us, however, consider the case below, where tasks A and B arrive at time 0, and tasks C, D, and E arrive at time 3. \n",
    "\n",
    "| Task | Start | Runtime (min)     |\n",
    "| :--: | :---: | :----------: |\n",
    "| A    | 0     | 2            |\n",
    "| B    | 0     | 4            |\n",
    "| C    | 3     | 1            |\n",
    "| D    | 3     | 1            |\n",
    "| E    | 3     | 1            |\n",
    "\n",
    "If we run jobs to completion we will get the following:\n",
    "\n",
    "```{figure} ../images/scheduling/SJF-1.png\n",
    "---\n",
    "name: VP:sched:SJF1\n",
    "---\n",
    "SJF without preemption\n",
    "```\n",
    "So the average turnaround time is: $(2 + 6 + 7 + 8 + 9)/3 = 10.7  min$\n",
    "\n",
    "If we instead preempt B to run the new jobs who's time is shorter than B's remaining time, we get the following execution:\n",
    "\n",
    "```{figure} ../images/scheduling/SJF-2.png\n",
    "---\n",
    "name: VP:sched:SJF2\n",
    "---\n",
    "SJF with preemption\n",
    "```\n",
    "With a shorter average turnaround time of $(2 + 4 + 5 + 6 + 9)/3 = 8.6$\n",
    "\n",
    "This demonstrates the value of preemption, where we can stop long running tasks to get short ones in and out of the system.  \n",
    "\n",
    "To understand the major problem with Shortest Job First, consider what happens if 1 minute tasks continue to arrive every minute starting at minute 7.  B will never complete, or in scheduling terminology, it will *starve* even though the system is processing work as fast as it is arriving.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201fbbe-3e7a-4f51-9ebe-8c368c9f5675",
   "metadata": {},
   "source": [
    "## Round Robin\n",
    "\n",
    "Our first preemptive scheduling algorithm is just like first come, first served but we have added the time slice so processes are no longer run to completion. In this model, we still have a single queue of processes in the Ready state, and processes can be added to it upon creation or when leaving the Blocked state. When a process becomes active, it is given a fixed amount of time to run, and when this time slice expires the OS interrupts the process and puts it at the back of the queue. \n",
    "\n",
    "```{Note}\n",
    "The preemptive scheduling models introduces a new parameter we need to set: the length of the time slice. We have to weigh the cost of changing processes against the interactivity requirements when deciding on the length of a time slice. Later on we will see systems that change the length based on usage patterns.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d8fa8-7530-43c6-a27a-a75f97b15d7a",
   "metadata": {},
   "source": [
    "## Priority\n",
    "\n",
    "The core idea behind priority scheduling is that some processes may be more important than others and should be given access to the CPU first. To implement this policy, the OS maintains two or more priority queues which hold processes assigned to the corresponding priority. Runnable processes in a higher priority queue are run before runnable processes in lower priority queues. In general we assign higher priorities to I/O bound processes and lower priorities to CPU bound ones. Figure {numref}`priority-sched` shows a simple example of a system with 4 priority queues. In this snapshot, the next process to be given CPU time will be the first process in the priority 4 queue. Assuming no additions, the processes in queue 3 will not run until all of the ones in 4 have completed.\n",
    "\n",
    "```{figure} ../images/scheduling/priority-sched.png\n",
    "---\n",
    "name: priority-sched\n",
    "---\n",
    "A simple example of a system with 4 priority queues and runnable processes in several of the queues.\n",
    "```\n",
    "\n",
    "```{Note}\n",
    "It may seem counter-intuitive to assign high priority to I/O bound processes because they often do not make use of their full time slice. We do this because a process that is frequently blocking on I/O is more likely to be interactive and therefore have a user who will notice latency when the scheduler ignores the process for several periods.\n",
    "```\n",
    "\n",
    "## Lottery\n",
    "\n",
    "As the name suggests, in lottery scheduling the OS gives 'tickets' to each runnable process. When the scheduler needs to select a new process to run, it picks a ticket at random and the process holding that ticket runs. With a small modification, we can express priority in this method by assigning more tickets to high priority processes than low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ac18a-3b66-4f3d-9e53-cb0a19124822",
   "metadata": {},
   "source": [
    "These examples are not exhaustive, there are other algorithms for selecting the next runnable process, however these examples are meant to illustrate that there are a number of ways to approach this problem and this is an active area of research today."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
